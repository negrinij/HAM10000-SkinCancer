{"cells":[{"metadata":{},"cell_type":"markdown","source":"![image.png](https://media.sciencephoto.com/image/p7100248/800wm/P7100248-SEM_of_section_through_human_skin.jpg)\n> Coloured scanning electron micrograph of a section through the human skin. <br>\nPhoto by <a href=\"https://www.sciencephoto.com/contributor/pzg/\">STEVE GSCHMEISSNER</a>\n  \n\n# HAM10000: Neural Networks for Skin Lesion Classification\n\n\n> \"Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset.\"<br>\n[Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data 5, 180161 doi:10.1038/sdata.2018.161 (2018)](https://www.nature.com/articles/sdata2018161).\n\n\nThis notebook explores the HAM10000 dataset and analyse the performance of different Neural Networks strategies. \n\n\n## <center style=\"background-color:Gainsboro; width:40%;\">Contents</center>\n1. [Overview](#1.-Overview)<br>\n1.1. [Content](#1.1.-Content)<br>\n1.2. [Acknowledgements](#1.2.-Acknowledgements)<br>\n2. [The Data](#2.-The-Data)<br>\n2.1 [Melanoma-Malignant Cancer Analysis](#2.-Melanoma-Malignant-Cancer-Analysis)<br>\n3. [Models](#3.-Models)<br>\n3.1 [CNN](#3.1-CNN)<br>\n3.2 [ResNet-50](#3.2-ResNet-50)<br>\n3.3 [XCeption](#3.3-XCeption)<br>\n4. [Results and Conclusion](#4.-Results-and-Conclusion)<br>\n\n***Please remember to upvote if you find this Notebook helpful!***"},{"metadata":{},"cell_type":"markdown","source":"# **1. Overview**\n\nDermatoscopy is a diagnostic technique that can improve the diagnosis of benign and malignant pigmented skin lesions. Other than increasing the accuracy of skin cancer detection (if compared to naked eye exams), dermatoscopic images can also be used to train ANN. In the past, promising attempts have been made to use ANN to classify skin lesions. However, the lack of data and computing power limited the application of this method.\n\nThe [ISIC archive](https://isic-archive.com/) is the largest public database for dermatoscopic image analysis research, and where the original HAM10000 was made available. In 2018, the database contained approximately 13.000 dermatoscopic images. Currently, the database holds over 60.000 images, demonstrating the power of collaboration between different scientific groups. \n\nAs mentioned by the authors, the original paper and release of HAM10000 aimed to boost the research on the automated diagnosis of dermatoscopic images. We can say they have certainly achieved that goal after three successful challenges and an impressive expansion of the database.\n\n\n## 1.1. Content ##\n\nThe HAM10000 dataset is composed of 10.015 dermatoscopic images of pigmented skin lesions. The data was collected from Australian and Austrian patients. Two institutions participated in providing the images: Cliff Rosendahl in Queensland, Australia, and Medical University of Vienna, Austria. According to the authors, seven classes are defined on this dataset where some diagnosis were unified into one class for simplicity. Information regarding patient age, sex, lesion location and diagnosis is also provided with each image.\n\n## 1.2. Acknowledgements ##\n\nThe dataset has been collated and published by [Tschandl, P., Rosendahl, C. & Kittler, H.](https://www.nature.com/articles/sdata2018161)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from numpy.random import seed\nseed(1)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport itertools\n\nimport keras\nfrom keras.applications import ResNet50, Xception\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,AveragePooling2D,GlobalMaxPooling2D\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras import regularizers\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. The Data\n\nIn this section, we analyse our metadata and understand a bit more regarding the patients and dataset distribution. \n\n## Key Insights\n\n* Small number of Missing Values, only for Age features where replacement with most frequent value was used\n* Similar distribution between Males and Females\n* Melanocytic nevi is the dominant class in the dataset (67%). It could result in a bias towards this type of os skin lesion\n* Most samples are from patients within 35 - 60 yrs old\n* Melanoma, malignant skin lesion, seems to be more common in the ages of 45 to 70. Males represent 62% of the incidence of this type of lesion\n\nA sample of each type of skin lesion present in the dataset is demonstrated in the chart below."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] / df.shape[0])*100))\n\ndef summary_table(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n\ndef countplot(df, x, x_axis_title,y_axys_title, plot_title):\n    plt.figure(figsize=(20,8))\n    sns.set(style=\"ticks\", font_scale = 1)\n    ax = sns.countplot(data = df,x=x,order = df[x].value_counts().index,palette=\"Blues_d\")\n    sns.despine(top=True, right=True, left=True, bottom=False)\n    plt.xticks(rotation=0,fontsize = 12)\n    ax.set_xlabel(x_axis_title,fontsize = 14,weight = 'bold')\n    ax.set_ylabel(y_axys_title,fontsize = 14,weight = 'bold')\n    plt.title(plot_title, fontsize = 16,weight = 'bold')  ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#Lesion Dictionary\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n\nbase_skin_dir = os.path.join('..', 'input')\n#Dictionary for Image Names\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(base_skin_dir, '*','*', '*.jpg'))}\n#Read File csv\nskin_df = pd.read_csv('../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n#Create useful Columns - Images Path, Lesion Type and Lesion Categorical Code\nskin_df['path'] = skin_df['image_id'].map(imageid_path_dict.get)\nskin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get) \nskin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"img = skin_df.sample(n=500,replace=False, random_state=1)\nimg['image'] = img['path'].map(lambda x: np.asarray(Image.open(x).resize((100,75))))\n#Image Sampling\nn_samples = 3\n\nfig, m_axs = plt.subplots(7, n_samples, figsize = (4*n_samples, 3*7))\n\nfor n_axs, (type_name, type_rows) in zip(m_axs,img.sort_values(['cell_type']).groupby('cell_type')):\n    n_axs[0].set_title(type_name)\n    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):\n        c_ax.imshow(c_row['image'])\n        c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The CSV file contains originally seven features. In the code lines above we add a few columns to support the data analysis and facilitate the extraction of images later on. The null entries are only related to the Age feature, as such no major data cleanse is required as the dataset is pretty much ready to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_EDA(skin_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table(skin_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The summary helps to understand the type of metadata collected. We noticed that for some lesions there must be more than one image as the lesion and image ID do not match. The Uniques columns also indicate the number of classes (dx = 7), and how the age, sex and localization features were organized.\n\nAll the features are pretty much self-explanatory. To clarify, the **dx_type** column is the technique used to identify the type of skin lesion.\n\nThe count plots below help to understand the distribution of the data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"countplot(skin_df,'dx_type', 'Diagnostic Type', 'Count', 'Samples per Type of Diagnosis')\ncountplot(skin_df,'cell_type', 'Type of Skin Lesion', 'Count', 'Samples per Type of Skin Lesion')\ncountplot(skin_df,'sex', 'Gender', 'Count', 'Samples per Gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The main thing to keep in mind is the unbalance between the different classes of skin lesion. Approximately 67% of data accounts for Melanocytic Nevi samples\n* The least represented classes are Dermatofibroma lesions and Vascular skin lesions, with only 115 and 142 samples, respectively\n* The samples are mostly Male participants, approximately 55%, not a significant difference between Genders"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"skin_df['age'].fillna((skin_df['age'].mode()), inplace=True)\n\nplt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = skin_df,x='age',palette=\"Blues_d\")\nsns.despine(top=True, right=True, left=True, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('Age',fontsize = 14,weight = 'bold')\nax.set_ylabel('Count',fontsize = 14,weight = 'bold')\nplt.title('Age Distribution', fontsize = 16,weight = 'bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The samples are predominantly from patients within 40 - 55 years old\n* The number of samples rises sharply after 25 years old, doubling the samples for 30 years old and almost doubling again for 35 years old\n* Between the ages of 60 - 70 years old the number of samples remain almost stable, returning to the downward trend after 75 years old"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Melanoma\n\n\nThe only malignant type of skin lesion present on this dataset is Malignant Melanoma, where surgical removal in the early stage of cancer can provide a cure. The remaining skin lesions are benign even though they may require treatment.\n\nIn this segment, we make a quick analysis of Melanoma skin lesions."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"skin_mel = skin_df.loc[:,['age','sex','localization','cell_type']]\nskin_mel = skin_mel[skin_mel['cell_type'] == 'Melanoma']\n\ncountplot(skin_mel,'sex', 'Gender', 'Count', 'Melanoma - Malignant Cancer per Gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Regarding gender, in the previous section, we saw a similar distribution across the main two genders. However, considering only Melanoma we can see from the above plot that more than 60% of the cases reported are Male"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = skin_mel,x='age',palette=\"Blues_d\", hue = 'sex')\nsns.despine(top=True, right=True, left=True, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('Age',fontsize = 14,weight = 'bold')\nax.set_ylabel('Count',fontsize = 14,weight = 'bold')\nplt.title('Age Distribution - Melanoma', fontsize = 16,weight = 'bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The age distribution is quite different for Melanoma diagnostics when compared to the whole dataset\n* For both genders, there are two distinct peaks, at 55 and 70 years old. The peaks could be related to the period people usually do a full health check-up\n* Younger Melanoma samples are more likely to be females. Impressive difference of gender for the ages between 25 - 40 years old\n* For males, melanoma has a higher incidence in older patients\n* After 40 years of age, the number of cases regarding female samples seems to stabilize. The peaks at 55 and 70 years old are not as expressive as the male samples"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"skin_local = skin_mel.groupby(['localization']).size().sort_values(ascending=False, inplace=False).reset_index()\nskin_local.columns = ['localization', 'count']\nsort_by = skin_local['localization']\n\nskin_heat = skin_mel.groupby(['age','localization']).size().reset_index()\nskin_heat.columns = ['age', 'localization', 'count']\nskin_heat.sort_values('count', ascending=False, inplace=True)\n\ndef heatmap(df, index,columns,values,vmax,sort_by,Title):\n    df_wide = df.pivot(index=index, columns=columns, values=values)\n    df_wide = df_wide.reindex(index=sort_by)\n    plt.figure(figsize=(12,8))\n    ax = sns.heatmap(df_wide, annot=True, fmt='.0f', yticklabels='auto', cmap=sns.color_palette(\"YlGnBu\", as_cmap=True), center=.2,vmin = 0, vmax = vmax,linewidths=.5)\n    ax.xaxis.tick_top() # x axis on top\n    ax.xaxis.set_label_position('top')\n    ax.set_xlabel(columns,fontsize = 14,weight = 'bold')\n    ax.set_ylabel(index,fontsize = 14,weight = 'bold')    \n    ax.set_title(Title,fontsize = 16,weight = 'bold',pad=20)\n    plt.show()\n    \nheatmap(skin_heat,'localization', 'age','count', 20,sort_by,'Age and Localization of Melanomas')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The heatmap makes a good representation of how age influences Cancer incidence. Note the cluster between the ages of 45 to 70\n* Back, upper and lower extremities are the most common locations of this melanoma. For the age group of 50 and 70 years old, the face, abdomen, chest and trunk also present a higher number of incidence.\n* The scalp seems to be a more common localization only for 70 years old. \n* The localizations do not seem to be related to the parts of the body most commonly exposed to the sun. If it was the case, scalp, hands and face should have a higher incidence"},{"metadata":{},"cell_type":"markdown","source":"# 3. Models\n\nHere we explore three different CNN networks. One manually built, ResNet-50 and XCeption. The data preparation for all of them is simple, and consists of:\n* Add the images to the Dataframe\n* Separate the dataframe into Features and Targets data\n* Create Training and Test sets (80 - 20 ratio)\n* Normalise the input. Following the best practices, the normalisation should be performed using the training set data as a reference. The test data cannot be normalised to its data, as it should remain unknown\n* One Hot Encoding to transform the Target labels\n* Separate the training set into Training and Validation sets (90 - 10 ratio)\n* The CNN requires the images to be reshaped into 3 dimensions (height = 75px, width = 100px , canal = 3)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"skin_df['image'] = skin_df['path'].map(lambda x: np.asarray(Image.open(x).resize((75,100))))\n\nfeatures=skin_df.drop(columns=['cell_type_idx'],axis=1)\ntarget=skin_df['cell_type_idx']\n\n# Create First Train and Test sets\nx_train_o, x_test_o, y_train_o, y_test_o = train_test_split(features, target, test_size=0.20,random_state=123)\n\n#The normalisation is done using the training set Mean and Std. Deviation as reference\nx_train = np.asarray(x_train_o['image'].tolist())\nx_test = np.asarray(x_test_o['image'].tolist())\n\nx_train_mean = np.mean(x_train)\nx_train_std = np.std(x_train)\n\nx_train = (x_train - x_train_mean)/x_train_std\nx_test = (x_test - x_train_mean)/x_train_std\n\n# Perform one-hot encoding on the labels\ny_train = to_categorical(y_train_o, num_classes = 7)\ny_test = to_categorical(y_test_o, num_classes = 7)\n\n#Splitting training into Train and Validatation sets\nx_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1,random_state=123)\n\n#Reshaping the Images into 3 channels (RGB)\nx_train = x_train.reshape(x_train.shape[0], *(75,100, 3))\nx_test = x_test.reshape(x_test.shape[0], *(75,100, 3))\nx_validate = x_validate.reshape(x_validate.shape[0], *(75,100, 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1 CNN\n\nThe Convolutional Neural Network is a specialised type of neural network, ideal for data that can be represented as a grid. CNN is most commonly used for image recognition tasks\nsince this input can be perceived as a 2D grid of pixels. As described by Goodfellow et al., (2016), CNN are neural networks that use at least one of their layers the convolution operation. For image classification tasks, Convolutional Neural Networks (CNN) are used most often. \n\nThe CNN architecture can vary and later we will explore well-known CNN models. In this section, it is presented a basic CNN architecture inspired by previous works and trial and error. \n\n* Average Pooling performed better than MaxPooling during my trials\n* Batch Normalization and Dropout seems to help avoid overfitting\n* A smaller pooling window (2,2) seems to perform better than larger kernel size (3,3) and (5,5)\n* Convolutional Layer > Activation Function > Batch Normalization > Dropout was the best combination I found \n\n>I understand that the original paper on Batch Normalization indicated the use of Batch before Activation, and I have tried such combination that reduced the accuracy by about 2%\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Model Parameters\ninput_shape = (75, 100, 3)\nnum_classes = 7\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\nepochs = 100\nbatch_size = 20\n\n#Callbacks\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=0, factor=0.5, min_lr=0.00001)\nearly_stopping_monitor = EarlyStopping(patience=20,monitor='val_accuracy')\n\n#Data Augmentation\ndataaugment = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0, # Randomly zoom image \n        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        shear_range = 10) \n\ndataaugment.fit(x_train)\n\ndef history(model):\n    model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    history = model.fit(dataaugment.flow(x_train,y_train, batch_size=batch_size),\n                        epochs = epochs, validation_data = (x_validate,y_validate),\n                        verbose = 0, steps_per_epoch=x_train.shape[0] // batch_size, \n                        callbacks=[learning_rate_reduction,early_stopping_monitor])\n\n    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n    predictions = model.predict(x_test)\n    loss_v, accuracy_v = model.evaluate(x_validate, y_validate, verbose=0)\n    loss_t, accuracy_t = model.evaluate(x_train, y_train, verbose=0)\n    return (predictions,accuracy_t,accuracy_v,accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))\nmodel.add(BatchNormalization())\n##############################\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n##############################\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n##############################\nmodel.add(Flatten())\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n#Output\nmodel.add(BatchNormalization())\nmodel.add(Dense(num_classes, activation='softmax'))\n\ny_pred, accuracy_t,accuracy_v,accuracy = history(model)\nprint(\"Training: accuracy = %f\" % (accuracy_t))\nprint(\"Validation: accuracy = %f\" % (accuracy_v))\nprint(\"Test: accuracy = %f\" % (accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The Accuracy values are not as high as one wishes for cancer prediction. However, there is only a small difference between training and evaluation sets, which is a good indicator that the model is not overfitting"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 ResNet50\n\nThe idea behind the residual networks (ResNet) is an attempt to overcome a problem faced by many researchers when working with deeper models where the training error starts to increase as more layers are added to the network. One hypothesis is that accuracy degradation occurs in deeper models because they are harder to optimize.\n\nThe introduction of deep residual learning was done by [He et al. 2015](arXiv:1512.03385v1). In the same year, this proposed architecture won first place in the most prestigious image recognition competitions, ILSVRC 2015 (ImageNet) and COCO 2015.\n\nFrom a quick literature review, it was noticed that ResNet was always among the pre-trained architectures used on the ISIC 2018 challenge. Here we use the ResNet-50, which uses 50 convolutional layers.\n\n* Best performance was achieved by training all layers of the ResNet50 and using the Imagenet dataset initial weights"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"base_model = ResNet50(include_top=False, input_shape=(75,100, 3),pooling = 'avg', weights = 'imagenet');\n\nResNet50model = Sequential()\nResNet50model.add(base_model)\nResNet50model.add(Dropout(0.2))\nResNet50model.add(Dense(128, activation=\"relu\"))\nResNet50model.add(Dropout(0.2))\nResNet50model.add(Dense(num_classes, activation = 'softmax'))\n###################################\n\nfor layer in base_model.layers:\n    layer.trainable = True\n\nResNet50y_pred,ResNet50accuracy_t,ResNet50accuracy_v,ResNet50accuracy = history(ResNet50model)\n    \nprint(\"ResNet50 Training: accuracy = %f\" % (ResNet50accuracy_t))\nprint(\"ResNet50 Validation: accuracy = %f\" % (ResNet50accuracy_v))\nprint(\"ResNet50 Test: accuracy = %f\" % (ResNet50accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* While the results vary according to the randomness of it all, ResNet50 usually performs better than the CNN model presented earlier. The test results vary within the range of 76-77% accuracy, similar to the CNN. However, when analysed after 5 runs, ResNet50 has a smaller standard deviation when compared to the CNN\n* Ideally, the models results should be an average across X number of runs. However,since this notebook is already taking almost one hour to run, I removed the additional runs from   the code"},{"metadata":{},"cell_type":"markdown","source":"# 3.3 XCeption\n\nThe Inception architectures, first introduced by Szegedy et al. in 2014 as GoogLeNet, and since then they have been the top-performing architectures on the ImageNet dataset.\n\nAccording to the author of XCeption paper, Francois Chollet, this architecture relies on the lessons learned with VGG-16 and previous Inception models. Also, they use residual connections (a concept introduced in ResNet) and depthwise separable convolutions inspired by the initial work of Vanhoucke (2014). In total, XCeption uses 39 Convolutional Layers.\n\n> \"XCeption is a linear stack of depthwise separable convolution layers with residual connections\" <br>\nChollet, F. (2017)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"training_shape = (75,100, 3)\nbase_model = Xception(include_top=False,weights='imagenet',input_shape = training_shape)\n\nXCeptionmodel = base_model.output\nXCeptionmodel = Flatten()(XCeptionmodel)\n\nXCeptionmodel = BatchNormalization()(XCeptionmodel)\nXCeptionmodel = Dense(128, activation='relu')(XCeptionmodel)\nXCeptionmodel = Dropout(0.2)(XCeptionmodel)\n\nXCeptionmodel = BatchNormalization()(XCeptionmodel)\nXCeptionoutput = Dense(num_classes, activation = 'softmax')(XCeptionmodel)\nXCeptionmodel = Model(inputs=base_model.input, outputs=XCeptionoutput)\n\nfor layer in base_model.layers:\n    layer.trainable = True\n\nXCeptiony_pred,XCeptionaccuracy_t,XCeptionaccuracy_v,XCeptionaccuracy = history(XCeptionmodel)\n    \nprint(\"XCeption Training: accuracy = %f\" % (XCeptionaccuracy_t))\nprint(\"XCeption Validation: accuracy = %f\" % (XCeptionaccuracy_v))\nprint(\"XCeption Test: accuracy = %f\" % (XCeptionaccuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Results and Conclusion\n\nFrom the three different architectures shown in this study, XCeption had an impressive better accuracy in the test set when compared to the other models. Even though the training and test set had a bigger accuracy gap, the XCeption model did not demonstrate major overfitting issues. As expected, the test set presented slightly lower values of accuracy for all networks.\n\nThe dataset was unbalanced, with the Melanocytic Nevi being the majority of the samples. For such cases, the accuracy metric can give us a false perception of the model reliability. Other metric scores (F1-Score, Precision, Recall) can provide a better idea of our model behaviour. \n\nFirst, the confusion matrix can give us a few insights."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predictions = np.array(list(map(lambda x: np.argmax(x), XCeptiony_pred)))\ncategories = ['Actinic keratoses', 'Basal cell carcinoma',\n              'Benign keratosis-like lesions ', \n              'Dermatofibroma', \n              'Melanocytic nevi',\n              'Melanoma', \n              'Vascular lesions']\n\nCMatrix = pd.DataFrame(confusion_matrix(y_test_o, predictions), columns=categories, index =categories)\n\nplt.figure(figsize=(12, 6))\nax = sns.heatmap(CMatrix, annot = True, fmt = 'g' ,vmin = 0, vmax = 20,cmap = 'Blues')\nax.set_xlabel('Predicted',fontsize = 14,weight = 'bold')\nax.set_xticklabels(ax.get_xticklabels(),rotation =90);\nax.set_ylabel('Actual',fontsize = 14,weight = 'bold')    \nax.set_title('Confusion Matrix - Test Set',fontsize = 16,weight = 'bold',pad=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the Confusion Matrix, the main interest was to evaluate how the Melanoma samples were being classified. An expressive number of samples were mistakenly classified as Melanocytic Nevi, almost the same number as the ones that were properly classified. It is not good that so many malign cancer samples are being classified as benign. For this reason, Accuracy is a dangerous metric for this application.\n\nNow, let's analyse the F1-Score results for each class. The F1 score takes into account precision and recall metrics, being more appropriate for unbalanced datasets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f1 = f1_score(y_test_o, predictions, average=None)\nindex = categories\nf1_df = pd.DataFrame(f1,index, columns = ['F1'])\nf1_df.sort_values(['F1'], ascending = False, inplace = True)\n\nplt.figure(figsize=(22,10))\nax = sns.barplot(data =f1_df, x=f1_df.index, y = 'F1',palette = \"Blues_d\")\n#Bar Labels\nfor p in ax.patches:\n        ax.annotate(\"%.1f%%\" % (100*p.get_height()), (p.get_x() + p.get_width() / 2., abs(p.get_height())),\n        ha='center', va='bottom', color='black', xytext=(-3, 5),rotation = 'horizontal',textcoords='offset points')\nsns.despine(top=True, right=True, left=True, bottom=False)\nax.set_xlabel('Skin Lesion Type',fontsize = 14,weight = 'bold')\nax.set_ylabel('F1-Score',fontsize = 14,weight = 'bold')\nax.set(yticklabels=[])\nax.axes.get_yaxis().set_visible(False) \nplt.title(\"F1-Score of XCeption Model for each Class\", fontsize = 16,weight = 'bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that for the class with the majority of samples we had a pretty good result. The nevi and vascular lesions classes are the ones that made the model accuracy reach values above 80%. For the least represented classes, we have a disappointing F1 Score, particularly for the Melanoma skin lesion.\n\nHow the results could be improved, and ideas for future Notebooks:\n\n* Optimisation methods to find optimal Data Augmentation, number of Convolutional layers and other hyperparameters. For this work, I mainly used Grid Search, which is not the most effective way to find hyperparameters\n* Use additional data. The ISIC website contains additional pictures that could be used to improve the detection of the least represented classes\n* In a similar topic, use GAN to generate more samples and improve model generalisation\n* This [Notebook](https://www.kaggle.com/akarsh1/skin-cancer-classification-with-85-02-accuracy) used the output of the CNN as inputs for Logistic Regression and other ML methods with exciting outcomes\n* As a final note, ensemble methods are always a good way to improve model accuracy"},{"metadata":{},"cell_type":"markdown","source":"## Please, remember to upvote if you found this useful :) [Kaggle Notebook](https://www.kaggle.com/jnegrini/ham10000-analysis-and-model-comparison)"},{"metadata":{},"cell_type":"markdown","source":"# References\n\nP. Tschandl, C. Rosendahl and H. Kittler, “The HAM10000 Dataset, a Large Collection of MultiSource Dermatoscopic Images of Common Pigmented Skin Lesions,” Scientific data, vol. 5, p. 180161, 2018.\n\nK. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” arXiv:1512.03385v1, 2015. \n\nI. Goodfellow, Y. Bengio and A. Courville, Deep Learning, Cambridge: MIT Press, 2016\n\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015\n\nV. Vanhoucke. Learning visual representations at scale. ICLR, 2014\n\nF., Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1251-1258). 2017"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}